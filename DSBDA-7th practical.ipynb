{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee1d75c5",
   "metadata": {},
   "source": [
    "                           Practical no:07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af63439",
   "metadata": {},
   "source": [
    "Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa94e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32d14edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8171fdbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e66acf3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27b4dfa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Welcome\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24d40522",
   "metadata": {},
   "outputs": [],
   "source": [
    " text= \"Tokenization is the first step in text analytics. The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41cbfd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization is the first step in text analytics.', 'The process of breaking down a text paragraph into smaller chunks such as words or sentences is called Tokenization.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize \n",
    "tokenized_text= sent_tokenize(text) \n",
    "print(tokenized_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b028e9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analytics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'words', 'or', 'sentences', 'is', 'called', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "tokenized_word=word_tokenize(text) \n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3cc6497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'shouldn', \"don't\", 'our', 'but', 'this', 'shan', 'most', 're', 'needn', 'because', 'as', 'both', \"i'd\", 'out', 't', \"wouldn't\", 'her', 'against', 'its', 'under', 'being', \"she'd\", \"he'll\", 'they', 'hadn', 'only', 'all', 'does', \"it'd\", 'so', 'themselves', 'to', 'until', \"weren't\", 'further', 'didn', \"i've\", 'when', 'then', \"you're\", \"that'll\", 'ain', 'having', \"he's\", \"they'd\", 'them', 'after', 'before', 'doing', 'will', \"she's\", 'now', 'whom', \"they've\", 'there', 'doesn', \"wasn't\", 'why', 'such', 'other', 'during', 'wasn', \"they'll\", \"we're\", 'am', 'ma', 'or', \"haven't\", \"hasn't\", 'any', 'she', \"we'll\", 've', 'below', 'into', 'very', 'above', \"you'll\", 'at', \"aren't\", 'm', \"i'm\", 'the', \"shouldn't\", 'couldn', 'what', 'too', 'him', 'yourselves', 'my', 'is', 'over', 'than', 'me', 'each', \"won't\", \"couldn't\", 's', \"hadn't\", 'you', 'nor', \"needn't\", 'ours', 'was', 'not', 'few', 'isn', 'some', 'again', 'had', 'an', \"didn't\", 'd', 'if', 'can', 'aren', 'in', 'weren', 'more', 'that', 'itself', 'myself', 'hers', 'he', 'should', 'about', 'have', 'who', 'it', 'for', 'no', 'down', 'been', 'be', 'by', 'his', 'we', 'which', 'himself', \"mightn't\", \"he'd\", 'once', 'those', 'don', 'on', 'with', 'between', \"it'll\", 'while', 'own', 'yours', 'how', \"mustn't\", \"should've\", 'll', 'and', 'y', 'hasn', 'same', \"shan't\", \"they're\", 'i', 'are', 'has', \"doesn't\", 'herself', 'theirs', 'won', 'wouldn', 'did', 'o', 'just', \"she'll\", 'these', 'yourself', \"you'd\", \"it's\", 'here', \"i'll\", \"isn't\", 'mightn', 'ourselves', \"we'd\", 'haven', 'a', 'their', 'were', 'mustn', 'through', 'up', 'where', \"we've\", 'your', 'off', \"you've\", 'of', 'from', 'do'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "stop_words=set(stopwords.words(\"english\")) \n",
    "print(stop_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59013e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b974df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Sentence: ['how', 'to', 'remove', 'stop', 'words', 'with', 'nltk', 'library', 'in', 'python']\n",
      "Filterd Sentence: ['remove', 'stop', 'words', 'nltk', 'library', 'python']\n"
     ]
    }
   ],
   "source": [
    "text= \"How to remove stop words with NLTK library in Python?\" \n",
    "text= re.sub('[^a-zA-Z]', ' ',text) \n",
    "tokens = word_tokenize(text.lower()) \n",
    "filtered_text=[] \n",
    "for w in tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_text.append(w) \n",
    "print(\"Tokenized Sentence:\",tokens) \n",
    "print(\"Filterd Sentence:\",filtered_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd26b2d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer \n",
    "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"] \n",
    "ps =PorterStemmer() \n",
    "for w in e_words: \n",
    "    rootWord=ps.stem(w) \n",
    "print(rootWord) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28f39ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemma for studies is study\n",
      "Lemma for studying is studying\n",
      "Lemma for cries is cry\n",
      "Lemma for cry is cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(\"Lemma for {} is {}\".format(w, wordnet_lemmatizer.lemmatize(w)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9897c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('pink', 'NN'), ('sweater', 'NN'), ('fit', 'VBP'), ('her', 'PRP$'), ('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data = \"The pink sweater fit her perfectly\"\n",
    "words = word_tokenize(data)\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f5b0d62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6c3e23ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "documentA = 'Jupiter is the largest Planet' \n",
    "documentB = 'Mars is the fourth planet from the Sun' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a6f39254",
   "metadata": {},
   "outputs": [],
   "source": [
    "bagOfWordsA = documentA.split(' ') \n",
    "bagOfWordsB = documentB.split(' ') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "492ba0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqueWords = set(bagOfWordsA).union(set(bagOfWordsB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32fb7503",
   "metadata": {},
   "outputs": [],
   "source": [
    "numOfWordsA = dict.fromkeys(uniqueWords, 0) \n",
    "for word in bagOfWordsA: \n",
    "    numOfWordsA[word] += 1 \n",
    "    numOfWordsB = dict.fromkeys(uniqueWords, 0) \n",
    "for word in bagOfWordsB: \n",
    "        numOfWordsB[word] += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "636f2ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF for Document A: {'the': 0.3333333333333333, 'pink': 0.16666666666666666, 'sweater': 0.16666666666666666, 'fit': 0.16666666666666666, 'her': 0.16666666666666666, 'perfectly': 0.16666666666666666}\n",
      "TF for Document B: {'the': 0.2, 'blue': 0.2, 'shirt': 0.2, 'fit': 0.2, 'her': 0.2}\n"
     ]
    }
   ],
   "source": [
    "def computeTF(wordDict, bagOfWords):\n",
    "    tfDict = {}\n",
    "    bagOfWordsCount = len(bagOfWords)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bagOfWordsCount)\n",
    "    \n",
    "    return tfDict\n",
    "\n",
    "numOfWordsA = {'the': 2, 'pink': 1, 'sweater': 1, 'fit': 1, 'her': 1, 'perfectly': 1}\n",
    "bagOfWordsA = ['the', 'pink', 'sweater', 'fit', 'her', 'perfectly']\n",
    "\n",
    "numOfWordsB = {'the': 1, 'blue': 1, 'shirt': 1, 'fit': 1, 'her': 1}\n",
    "bagOfWordsB = ['the', 'blue', 'shirt', 'fit', 'her']\n",
    "\n",
    "tfA = computeTF(numOfWordsA, bagOfWordsA)\n",
    "tfB = computeTF(numOfWordsB, bagOfWordsB)\n",
    "print(\"TF for Document A:\", tfA)\n",
    "print(\"TF for Document B:\", tfB)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9ee146c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f33cfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pink': 0.6931471805599453, 'the': 0.0, 'her': 0.0, 'sweater': 0.6931471805599453, 'shirt': 0.6931471805599453, 'perfectly': 0.6931471805599453, 'blue': 0.6931471805599453, 'fit': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def computeIDF(documents):\n",
    "    N = len(documents)\n",
    "    all_words = set(word for document in documents for word in document.keys())\n",
    "    idfDict = dict.fromkeys(all_words, 0)\n",
    "    for document in documents:\n",
    "        for word in document:\n",
    "            if document[word] > 0: \n",
    "                idfDict[word] += 1\n",
    "        for word, val in idfDict.items():\n",
    "        if val > 0:  \n",
    "            idfDict[word] = math.log(N / float(val))\n",
    "    \n",
    "    return idfDict\n",
    "numOfWordsA = {'the': 2, 'pink': 1, 'sweater': 1, 'fit': 1, 'her': 1, 'perfectly': 1}\n",
    "numOfWordsB = {'the': 1, 'blue': 1, 'shirt': 1, 'fit': 1, 'her': 1}\n",
    "idfs = computeIDF([numOfWordsA, numOfWordsB])\n",
    "print(idfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bd2fad52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   the      pink   sweater  fit  her  perfectly      blue     shirt\n",
      "0  0.0  0.067307  0.067307  0.0  0.0   0.067307       NaN       NaN\n",
      "1  0.0       NaN       NaN  0.0  0.0        NaN  0.081093  0.081093\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def computeTFIDF(tfBagOfWords, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBagOfWords.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    \n",
    "    return tfidf\n",
    "tfA = {'the': 0.333, 'pink': 0.166, 'sweater': 0.166, 'fit': 0.166, 'her': 0.166, 'perfectly': 0.166}\n",
    "tfB = {'the': 0.2, 'blue': 0.2, 'shirt': 0.2, 'fit': 0.2, 'her': 0.2}\n",
    "\n",
    "idfs = {'the': 0.0, 'pink': 0.405465, 'sweater': 0.405465, 'fit': 0.0, 'her': 0.0, 'perfectly': 0.405465, 'blue': 0.405465, 'shirt': 0.405465}\n",
    "tfidfA = computeTFIDF(tfA, idfs)\n",
    "tfidfB = computeTFIDF(tfB, idfs)\n",
    "df = pd.DataFrame([tfidfA, tfidfB])\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a447b31e",
   "metadata": {},
   "source": [
    "Name- Nitesh Lad\n",
    "\n",
    "Roll_No-13224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aec8603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
